{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The N-armed bandit problems\n",
    "\n",
    "Let's look at a famous reinforcement learning problem called the N-armed bandit problem. A client in a casino, has access to several levers, each one yielding money with a different distribution of probabilities. Given that the client has no prior knowledge of these distributions, he must tries each lever several times before he can infer which one is best. Of course, this involves a tradeoff between trying a lever whose payoff is uncertain (a.k.a exploring) and repeatedly using the lever that yields the highest return in average.\n",
    "\n",
    "This seemingly simple problem is a classic in the machine learning field and many nontrivial algorithm were devised to find an operational solution to it. Inspiration for the examples below example was drawn from *Reinforcement Learning:\n",
    "An Introduction*, a book by Richard S. Sutton and Andrew G. Barto, which is also available as [html](http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)\n",
    "\n",
    "Here, I consider only two very simple strategies, namely Monte-Carlo simulations and an $\\epsilon$-greedy strategy. Their performance is assessed by running a large number of simulations for each of them. The programming challenge will be to write a Python program whose computation time does not increase to fast so as to make this comparison feasible. Luckily, Python has many available options to optimize optimize. We will use this learning exercise to review two of them: vectorization and Just-In-Time copmilation.\n",
    "\n",
    "[This notebook was initially conceived as an interactive exercise for the Workshop on High Performance Computing that took place before the CEF 2014 conference in [Oslo](http://comp-econ.org/CEF_2014/Oslo). Each bullet point was meant as a question to solve by participant. You can download the initial exercise without the answers from here : [give link]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it yourself !\n",
    "\n",
    "If you run this notebook on your computer, and if you drop the `n_armed_bandit_game.py`, in the same directory as the notebook, you can try to play the N-armed bandit game yourself.\n",
    "\n",
    "Click each button in turn and try to maximize the total discounted value. The value reported next to each button $i$ is the average of the past payoffs for bandit $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec218ec6a4043dc958d2e2fd1e52122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Button(description='Bandit 0', style=ButtonStyle()), HTML(value='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from n_armed_bandit_game import HumanPlayer\n",
    "hp = HumanPlayer(5)\n",
    "hp.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the problem\n",
    "\n",
    "Let's define the problem more formally.\n",
    "\n",
    "Take N random distributions $\\left(d(n)\\right)_{n\\in[1,N]}$. Each distribution is called an arm of the machine.\n",
    "\n",
    "The Player faces a machine with $N$ arms. Each arm $n\\in[1,N]$ gives a stochastic payoff $d(n)$ which is i.i.d.\n",
    "\n",
    "The distribution of the rewards is unknown but at each date $t\\in [1,T]$ the player can take an action $n$ and get the stochastic payoff $d(n)$.\n",
    "\n",
    "His goal is to choose a series of actions $(a_t)_{0\\leq t \\leq T}$  at each time to maximize intertemporal utility $\\sum_{0 \\leq t \\leq T} \\beta^t U(d_{a_t})$ with $0<\\beta<1$ and $U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}$ with $\\gamma>1$.\n",
    "\n",
    "The player doesn't know the distributions of reward of each arm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this tutorial is to implement to simple learning strategy. \n",
    "- a stupid Monte-Carlo strategy\n",
    "- an $\\epsilon$ greedy strategy where the optimal action is selected with probability $1-\\epsilon$ while a random action is taken with probability $\\epsilon$\n",
    "\n",
    "Each strategy will be given $K=1000$ different randomly drawn games to play, each of them being a $T=10000$ periods learning episode. Then we will average over the $K$ experiments to which is the best strategy. In particular, we want to find the value of $\\epsilon$ that yields the optimal exploration/exploitation tradeoff for our distribution of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the $d(n)$ are normally distributed with standard deviation $1$. The means $(m_n)$ of these distribution are a permutation of the equally distributed points $\\left(\\frac{n-1}{N-1}\\right)_{n\\in[1,N]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we choose the parameters:\n",
    "\n",
    "T = 1000\n",
    "K = 10000\n",
    "\n",
    "N = 5        # number of arms\n",
    "beta = 0.95   # discount factor\n",
    "gamma = 0.5   # risk aversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- draw an instance of the problem as two one-dimensional arrays of size N, `means` and `deviations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Best Arm with Monte-Carlo simulations (and use vectorized computations)\n",
    "\n",
    "Here we consider a very simple strategy: \n",
    "- Given an integer $T_1$. During $N T_1$ periods, we use each lever in turn in order to estimate the lever that yields the higher utility.\n",
    "- Then, until last date $T$, we us the lever for which expected utility is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T1 = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm can be expressed in a way that doesn't require any conditional operation. The first and the second step can be expressed as a series of array operations. \n",
    "\n",
    "----\n",
    "- create an array `'rewards'` of size $N \\times T_1$ such that each line $n$ contains $T_1$ realizations of the distribution $d(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- estimate the mean utility procured by each arm and select the best arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- compute the estimated mean of each distribution and the discounted value of playing each arm in turn during $N T_1$ periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- compute the value of the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To profile the efficiency of these computations, the easiest way consists in encapsulate them in a function and profile this functions using IPython's `prun` magic.\n",
    "\n",
    "----\n",
    "- encapsulate the algorithm as a function and profile it (use ipython magic `%prun -s time`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- Find the optimal number of periods to spend on monte-carlo evaluations (plot results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\epsilon$-greedy strategies\n",
    "\n",
    "We now consider a more elaborate strategy. Like in the former algorithm, we maintain a guess for the expected reward of each arm.\n",
    "But at each step we choose between \"exploiting\" the best option or \"exploring\" another one at random.\n",
    "\n",
    "Here is the definition of the algorithm. At each step:\n",
    "\n",
    "At each step: \n",
    "- take an action\n",
    " - with probability $1-\\epsilon$, choose the arm that is believed to have higher return (greedy option)\n",
    " - with probability $\\epsilon$ take another arm at random    \n",
    "- Update the expection of the chosen option.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- implement the epsilon-greedy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- compute the average return over $K$ experiments for $\\epsilon=0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aouch, this naive approach took approximately one minute to complete... But we can certainly do better. As we did before, we will try to locate bottlenecks by profiling the code.\n",
    "\n",
    "----\n",
    "- profile using prun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- optimize speed of function play using numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to vary the exploration parameter $\\epsilon$ to see if the $\\epsilon$-greedy can perform better than the very simple Monte-Carlo strategy. \n",
    "\n",
    "----\n",
    "- Find the epsilon that maximzes average value over K simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
